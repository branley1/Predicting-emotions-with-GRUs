{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de6dde-e0c2-47fd-8f22-d268bcb29403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dense, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from aitk.utils import gallery, array_to_image\n",
    "from aitk.networks import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afd4d0-3191-43fa-874f-7cd6314142a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load file; takes in a string and returns an array of data\n",
    "def loadFile(filename):\n",
    "    array = []\n",
    "    try:\n",
    "        df = pd.read_csv(filename, nrows=70000)\n",
    "        array = df.to_numpy()\n",
    "    except:\n",
    "        print(\"Invalid File!\")\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716c854-04e1-4717-882d-929c8ffe875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section of the code will prepare the dataset\n",
    "\n",
    "#maps numerical labels into lexical labels\n",
    "label_map = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "\n",
    "#Hyperparameters\n",
    "#number of different types of words\n",
    "vocab_size = 5000\n",
    "#length of vector of each words after embedding\n",
    "embd_len = 32\n",
    "#maximum input size; every input has padding to ensure all inputs have the same size\n",
    "max_words = 100\n",
    "\n",
    "#replace with path to text.csv file\n",
    "filename = \"~/Desktop/text.csv\"\n",
    "#calls loadfile with filename; returns array with index, tweets, and labels\n",
    "loadedFile = loadFile(filename)\n",
    "\n",
    "#get tweets from loaded file\n",
    "allText = [tweet[1] for tweet in loadedFile]\n",
    "\n",
    "#code to turn text into a unique number (token)\n",
    "allTextTokenizer = Tokenizer(num_words = vocab_size)\n",
    "allTextTokenizer.fit_on_texts(allText)\n",
    "allText = allTextTokenizer.texts_to_sequences(allText)\n",
    "\n",
    "#dictionary mapping words to tokens\n",
    "wordDict = allTextTokenizer.word_index\n",
    "\n",
    "#pad inputs so they are all the same size\n",
    "allText = sequence.pad_sequences(allText, maxlen = max_words)\n",
    "\n",
    "#get labels from loaded file\n",
    "allLabels = [tweet[2] for tweet in loadedFile]\n",
    "\n",
    "#turn each label into a vector of length six: (1, 0, 0, 0, 0, 0) = sadness for example\n",
    "allLabels_category = to_categorical(allLabels, 6)\n",
    "\n",
    "#separate data into training and testing sets\n",
    "text_train = np.array(allText[:60000])\n",
    "label_train = np.array(allLabels_category[:60000])\n",
    "text_test = np.array(allText[60000:70000])\n",
    "label_test = np.array(allLabels_category[60000:70000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008bbf9-0c09-4fe2-b521-4a5c896377c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section of the code creates the first model, which uses a GRU and trains on 10000 inputs\n",
    "\n",
    "#Create a sequential model\n",
    "gru_model = Sequential(name=\"GRU_Model\")\n",
    "\n",
    "#Add an embedding layer which takes an input (list of tokens) and turn each token in input into a vector of length 32\n",
    "gru_model.add(Embedding(vocab_size,\n",
    "                        embd_len,\n",
    "                        input_length=max_words))\n",
    "\n",
    "#Add GRU layer with hyperbolic tangent activation\n",
    "gru_model.add(GRU(128,\n",
    "                activation='tanh',\n",
    "                return_sequences=False))\n",
    "\n",
    "#Add FC layer with six outputs, each represnting an emotion\n",
    "gru_model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "#Printing the summary\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f926809-330e-45b3-93f0-e3583d9b8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section of the code compiles and trains the first model\n",
    "\n",
    "#Compiling the model\n",
    "gru_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "net = Network(gru_model)\n",
    "\n",
    "#Training the GRU model\n",
    "history = net.fit(text_train[:10000], label_train[:10000],\n",
    "                        batch_size=256,\n",
    "                        epochs=5,\n",
    "                        verbose=1,\n",
    "                        validation_data=(text_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee121ca9-42ab-407e-8c4f-11f7f0ae28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing first model score on test data\n",
    "print(\"GRU model 1 Score---> \", gru_model.evaluate(text_test, label_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0450221-0c0c-456f-8e91-cf2914253fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section of the code creates the second model, which uses a GRU and trains on 60000 inputs\n",
    "\n",
    "gru_model2 = Sequential(name=\"GRU_Model2\")\n",
    "gru_model2.add(Embedding(vocab_size,\n",
    "                        embd_len,\n",
    "                        input_length=max_words,\n",
    "                        name=\"embedding\"))\n",
    "gru_model2.add(GRU(128,\n",
    "                activation='tanh',\n",
    "                return_sequences=False))\n",
    "gru_model2.add(Dense(6, activation='softmax'))\n",
    "\n",
    "gru_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4405b573-4b2e-4d4f-9eab-2cd00373960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section of the code compiles and trains the second model\n",
    "\n",
    "gru_model2.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "net2 = Network(gru_model2)\n",
    "\n",
    "history = net2.fit(text_train, label_train,\n",
    "                        batch_size=256,\n",
    "                        epochs=5,\n",
    "                        verbose=1,\n",
    "                        validation_data=(text_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f349ea5-f982-4cf1-8d54-3334172a11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing second model score on test data\n",
    "print(\"GRU model 2 Score---> \", gru_model2.evaluate(text_test, label_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb8dee-b641-4977-bee0-05132b712f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing second model score on test data with incorrect labels\n",
    "print(\"GRU model 2 Score---> \", gru_model2.evaluate(text_test, label_train[0:10000], verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd841ce3-d3df-42c3-b02d-7c96c56aaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section of the code creates the third model, which uses a Simple RNN and trains on 60000 inputs\n",
    "\n",
    "rnn_model = Sequential(name=\"RNN_Model\")\n",
    "rnn_model.add(Embedding(vocab_size,\n",
    "                        embd_len,\n",
    "                        input_length=max_words))\n",
    "rnn_model.add(SimpleRNN(128,\n",
    "                activation='tanh',\n",
    "                return_sequences=False))\n",
    "rnn_model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123eefd4-5686-47d0-a689-4b69c7ecacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section of the code compiles and trains the third model\n",
    "\n",
    "rnn_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "net3 = Network(rnn_model)\n",
    "\n",
    "history = net3.fit(text_train, label_train,\n",
    "                        batch_size=256,\n",
    "                        epochs=5,\n",
    "                        verbose=1,\n",
    "                        validation_data=(text_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b991365-3f79-46ba-a20b-714366c67ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing second model score on test data with incorrect labels\n",
    "print(\"RNN model Score---> \", rnn_model.evaluate(text_test, label_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399a433-3213-4686-8876-39840e761796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section of the code looks at the embeddings of tokens in the second model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#function that takes in a float array and create a colormap; use to compare different embeddings\n",
    "def convertToColorMap(float_array):\n",
    "    normalize = [(data - np.min(float_array))/(np.max(float_array) - np.min(float_array)) for data in float_array]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow([normalize], cmap='Greens', aspect='auto')\n",
    "    ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "#gets the weights of every token (word) in vocabulary\n",
    "weights = gru_model2.get_layer('embedding').get_weights()[0]\n",
    "\n",
    "#displays the colormap of a specific word\n",
    "convertToColorMap(weights[wordDict[\"unhappy\"]])\n",
    "convertToColorMap(weights[wordDict[\"depressed\"]])\n",
    "convertToColorMap(weights[wordDict[\"sad\"]])\n",
    "convertToColorMap(weights[wordDict[\"sadness\"]])\n",
    "convertToColorMap(weights[wordDict[\"happy\"]])\n",
    "convertToColorMap(weights[wordDict[\"happiness\"]])\n",
    "\n",
    "#To see a different embedding, copy and past the following code: convertToColorMap(weights[wordDict[\"word\"]]) and replace 'word'\n",
    "#with whatever you want. Note that the vocab size is only 5000, so some words may give you an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f4c36-3e6e-41fc-8860-12bd03679990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section and all sections below are used to generate results from the model\n",
    "\n",
    "from numpy import argmax\n",
    "\n",
    "#inputs the test data into model and returns the results\n",
    "outputs = net2.predict(text_test)\n",
    "\n",
    "#array of predicted answers\n",
    "answers = [argmax(output) for output in outputs]\n",
    "\n",
    "#array of correct answers\n",
    "targets = [argmax(target) for target in label_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19162186-2f5f-44f0-9703-d7d4c82c0ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks the number of incorrect answers (predicted answer does not equal actual answer)\n",
    "incorrect = [i for i in range(len(answers)) if answers[i] != targets[i]]\n",
    "len(incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35422d-efad-4e53-9b62-632544e5f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a confusion matrix to see where the model is likely to mess up\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "%matplotlib widget\n",
    "\n",
    "cm = confusion_matrix(targets, answers)\n",
    "cm_plt = ConfusionMatrixDisplay(cm, display_labels=label_map)\n",
    "cm_plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
